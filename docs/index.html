<html>
	<head>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
	<script src = "assets/jquery.fullPage.js"></script>
	<link rel="stylesheet" type="text/css" href="assets/styles.css" />
	<link rel="stylesheet" href = "assets/jquery.fullPage.css"/>
	<script src = "assets/scripts.js"></script>
	<link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
	</head>

	<body>
		<div id = "fullpage">
			<div id="sec1" class="section">
				<div id="page1">
					<div id='content'>
						<canvas id="myCanvas" width="400" height="400">
Your browser does not support the HTML canvas tag.
</canvas><br>
  <audio id="audio" controls></audio>
  <label id="upload" for="thefile">Choose a file</label>
  <input type="file" id="thefile" accept="audio/*" /><br>
<script src="https://twgljs.org/dist/twgl.min.js"></script>
<script id="vshader" type="whatever">
	attribute vec4 a_position;
    varying vec2 v_texcoord;
    void main() {
      gl_Position = a_position;

      // assuming a unit quad for position we
      // can just use that for texcoords. Flip Y though so we get the top at 0
      v_texcoord = a_position.xy * vec2(0.5, -0.5) + 0.5;
    }
</script>
<script id="fshader" type="whatever">
precision mediump float;
varying vec2 v_texcoord;
uniform sampler2D u_image;
uniform sampler2D u_palette;

void main() {
    float index = texture2D(u_image, v_texcoord).a * 255.0;
    gl_FragColor = texture2D(u_palette, vec2((index + 0.5) / 256.0, 0.5));
}
</script>
<script>
const input = document.querySelector('input');
var c = document.getElementById("myCanvas");
var gl = c.getContext("webgl");
var file = document.getElementById("thefile");
var audio = document.getElementById("audio");
var N = 200;
var stage = 0;
var prev = 0;
var closed = false;

var dens = getArray();
var densB = getArray();
var velX = getArray();
var velY = getArray();
var tempA = getArray();
var tempB = getArray();

var program = twgl.createProgramFromScripts(
    gl,
    ["vshader", "fshader"],
    ["a_position", "a_textureIndex"]);
gl.useProgram(program);
var imageLoc = gl.getUniformLocation(program, "u_image");
var paletteLoc = gl.getUniformLocation(program, "u_palette");
// tell it to use texture units 0 and 1 for the image and palette
gl.uniform1i(imageLoc, 0);
gl.uniform1i(paletteLoc, 1);

// Setup a unit quad
var positions = [
      1,  1,
     -1,  1,
     -1, -1,
      1,  1,
     -1, -1,
      1, -1,
];
var vertBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, vertBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);
gl.enableVertexAttribArray(0);
gl.vertexAttribPointer(0, 2, gl.FLOAT, false, 0, 0);

function IX(i, j) {
	return ((i) + ((N+2)*(j)));
}

function getArray() {
	return new Float32Array((N + 2) * (N + 2));
}

c.onmousedown = function(e) {
	var m = 4;
	var vecX = m * ((N/2) - (e.clientX / 2));
	var vecY = m * ((N/2) - (e.clientY / 2));
	for (let i = -10; i <= 10; i++) {
		for (let j = -10; j <= 10; j++) {
			velX[IX(Math.floor(e.clientX / 2) + i, Math.floor(e.clientY / 2) + j)] = vecX;
			velY[IX(Math.floor(e.clientX / 2) + i, Math.floor(e.clientY / 2) + j)] = vecY;
			//dens[IX(Math.floor(e.clientX / 2) + i, Math.floor(e.clientY / 2) + j)] = 1;
		}
	}
}

function diff(arr, arrN, b, a) {

	for (let k = 0; k < 20; k++) {
		for (let i = 1; i <= N; i++) {
			for (let j = 1; j <= N; j++) {
				arrN[IX(i,j)] = (arr[IX(i,j)] + (a * (arrN[IX(i-1,j)] + arrN[IX(i+1,j)] + arrN[IX(i,j-1)] + 5*arrN[IX(i,j+1)]))) / (1 + 8 * a);
			}
		}
		bounds(arrN, b);
	}
}

function advect(arr, arrN, b) {
	var x;
	var y;
	var i0;
	var i1;
	var j0;
	var j1;
	var s1;
	var s0;
	var t1;
	var t0;
	for (let i = 1; i <= N; i++) {
		for (let j = 1; j <= N; j++) {
			x = i - velX[IX(i, j)];
			y = j - velY[IX(i, j)];
			if (x < 0.5) {
				x = 0.5;
			}
			if (y < 0.5) {
				y = 0.5;
			}
			if (x > N + 0.5) {
				x = N + 0.5;
			}
			if (y > N + 0.5) {
				y = N + 0.5;
			}
			i0 = Math.floor(x);
			i1 = i0 + 1;
			j0 = Math.floor(y);
			j1 = j0 + 1;
			j1 = j0 + 1;
			s1 = x - i0;
			s0 = 1 - s1;
			t1 = y - j0;
			t0 = 1 - t1;
			arrN[IX(i, j)] =
		(s0 * ((t0 * arr[IX(i0, j0)]) + (t1 * arr[IX(i0, j1)]))) +
		(s1 * ((t0 * arr[IX(i1, j0)]) + (t1 * arr[IX(i1, j1)])));
		}
	}
	bounds(arrN, b);
}

function project() {
	var h = 1.0 / N;
	var div = getArray();
	var pres = getArray();

	for (let i = 1; i <= N; i++) {
		for (let j = 1; j <= N; j++) {
			div[IX(i,j)] = -0.5 * (velX[IX(i+1,j)] - velX[IX(i-1,j)] + velY[IX(i,j+1)] - velY[IX(i,j-1)]);
			pres[IX(i,j)] = 0.0;
		}
	}
	bounds(div, 0);
	bounds(pres, 0);

	for (let k = 0; k < 20; k++) {
		for (let i = 1; i <= N; i++) {
			for (let j = 1; j <= N; j++) {
				pres[IX(i,j)] = (div[IX(i,j)] + pres[IX(i-1,j)] + pres[IX(i+1,j)] + pres[IX(i,j-1)] + pres[IX(i,j+1)]) / 4.0;
			}
		}
		bounds(pres, 0);
	}

	for (let i = 1; i <= N; i++) {
		for (let j = 1; j <= N; j++) {
			velX[IX(i,j)] -= (0.5 * (pres[IX(i+1,j)] - pres[IX(i-1,j)]));
			velY[IX(i,j)] -= (0.5 * (pres[IX(i,j+1)] - pres[IX(i,j-1)]));
		}
	}
	bounds(velX, 1);
	bounds(velY, 2);
}

function bounds(arr, b) {
	for (let i = 1; i <= N; i++) {
		if (b == 1) {
			arr[IX(0,i)] = -1 * arr[IX(1, i)];
		} else {
			arr[IX(0,i)] = arr[IX(1, i)];
		}
		if (b == 1) {
			arr[IX(N+1,i)] = -1 * arr[IX(N, i)];
		} else {
			arr[IX(N+1,i)] = arr[IX(N, i)];
		}
		if (closed) {
			if (b == 2) {
			arr[IX(i,0)] = -1 * arr[IX(i, 1)];
		} else {
			arr[IX(i,0)] = arr[IX(i, 1)];
		}
		} else {
			arr[IX(i,0)] = 0;
		}
		if (b == 2) {
			arr[IX(i,N+1)] = -1 * arr[IX(i, N)];
		} else {
			arr[IX(i,N+1)] = arr[IX(i, N)];
		}
	}
	arr[IX(0  ,0  )] = 0.5 * (arr[IX(1,0  )] + arr[IX(0  ,1)]);
	arr[IX(0  ,N+1)] = 0.5 * (arr[IX(1,N+1)] + arr[IX(0  ,N)]);
	arr[IX(N+1,0  )] = 0.5 * (arr[IX(N,0  )] + arr[IX(N+1,1)]);
	arr[IX(N+1,N+1)] = 0.5 * (arr[IX(N,N+1)] + arr[IX(N+1,N)]);
}

function dens_step() {
	tempA = getArray();
	diff(dens, tempA, 0, 0.9);
	dens = tempA;
	tempA = getArray();
	advect(dens, tempA, 0);
	dens = tempA;
}

function vel_step() {
	tempA = getArray();
	tempB = getArray();
	diff(velX, tempA, 1, 0.25);
	diff(velY, tempB, 2, 0.25);
	velX = tempA;
	velY = tempB;
	project();
	tempA = getArray();
	tempB = getArray();
	advect(velX, tempA, 1);
	advect(velY, tempB, 2);
	velX = tempA;
	velY = tempB;
	project();
}

file.onchange = function() {
    var files = this.files;
    audio.src = URL.createObjectURL(files[0]);
    audio.load();
    audio.play();
    var context = new AudioContext();
    var src = context.createMediaElementSource(audio);
    var analyser = context.createAnalyser();

    src.connect(analyser);
    analyser.connect(context.destination);

    analyser.fftSize = 32;

    var bufferLength = analyser.frequencyBinCount;

    var dataArray = new Uint8Array(bufferLength);
    var prevArray = new Uint8Array(bufferLength);
    var maxArray = new Uint8Array(bufferLength);
    var incCount = 0;
    var prevSum = 0;

	function frame(t) {

		analyser.getByteFrequencyData(dataArray);
		var sum = 0;
		for (let i = 0; i < 4; i++) {
			sum += dataArray[i] * dataArray[i];
		}
		console.log(closed);
		if ((sum - prevSum > 20000) || (sum > 170000 && prevSum < 170000)) {
			stage += 1;
			for (let k = -4; k <= 4; k++) {
				for (let j = -8; j <= 8; j++) {
					velY[IX(10 + k, 160 + j)] = -8;
					velX[IX(10 + k, 160 + j)] = 8;
					velY[IX(190 + k, 160 + j)] = -8;
					velX[IX(190 + k, 160 + j)] = -8;
				}
			}
		}
		prevSum = sum;
		for (let i = 0; i < bufferLength; i++) {
			var centerX = Math.floor((i + 1.0) * ((N / (bufferLength)) + 4));
			var centerY = 190;
			if (dataArray[i] - prevArray[i] > 2.0) {
			// if (dataArray[i] - prevArray[i] > 2.0) {
        // var diff = (dataArray[i] - prevArray[i]) / 20.0;
				var m = Math.floor(8 - (6 * (i / bufferLength)));
				for (let k = -m; k <= m; k++) {
					for (let j = -m; j <= m; j++) {
						dens[IX(centerX + k, centerY + j)] = (1.0) * dataArray[i] / 256.0;
						velY[IX(centerX + k, centerY + j - 10)] = (-5.0) * (dataArray[i] / 256);
					}
				}

        // centerX = 200;
        if (dataArray[i] - prevArray[i] > 10.0) {

          var ratio = dataArray[i] / 255;

          centerY = 200;

          var vecX = ratio * 1.5 * ((N/2) - (centerX / 2));
          var vecY = ratio * 5.0 * ((N/2) - (centerY / 2));

          var square = Math.floor(ratio * 7);
          // console.log(square);
          for (let i = -square; i <= square; i++) {
            for (let j = -square; j <= square; j++) {
              velX[IX(centerX + i, centerY +5 + j)] += vecX;
              velY[IX(centerX + i, centerY +5 + j)] += vecY;
              }
          }
        }
			}
			prevArray[i] = dataArray[i];
		}

		dens_step();
		vel_step();
		//225	0	245
		var colors = [[225, 0, 245], [255, 145, 26], [254, 50, 24]];
		var palette = new Uint8Array(256 * 4);
		for (let i = 0; i < 256; i++) {
			// palette[(i * 4) + 0] = i * Math.max((Math.floor(stage) % 3), 1);
			// palette[(i * 4) + 1] = i * Math.max(((Math.floor(stage) + 1) % 3), 1);
			// palette[(i * 4) + 2] = i * Math.max(((Math.floor(stage) + 2) % 3), 1);
			var cur_color = colors[Math.floor(stage) % 3];
			palette[(i * 4) + 0] = i * cur_color[0] / 256.0;
			palette[(i * 4) + 1] = i * cur_color[1] / 256.0;
			palette[(i * 4) + 2] = i * cur_color[2] / 256.0;
			palette[(i * 4) + 3] = i;
		}

		gl.activeTexture(gl.TEXTURE1);
		var paletteTex = gl.createTexture();
		gl.bindTexture(gl.TEXTURE_2D, paletteTex);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
		gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, 256, 1, 0, gl.RGBA, gl.UNSIGNED_BYTE, palette);

		var image = new Uint8Array(N * N);
		for (let i = 1; i <= N; i++) {
			for (let j = 1; j <= N; j++) {
				image[i + (N * j)] = Math.floor(255 * dens[IX(i,j)]);
			}
		}

		// make image textures and upload image
		gl.activeTexture(gl.TEXTURE0);
		var imageTex = gl.createTexture();
		gl.bindTexture(gl.TEXTURE_2D, imageTex);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
		gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
		gl.texImage2D(gl.TEXTURE_2D, 0, gl.ALPHA, N, N, 0, gl.ALPHA, gl.UNSIGNED_BYTE, image);

		gl.drawArrays(gl.TRIANGLES, 0, positions.length / 2);
		window.requestAnimationFrame(frame);
	}

    audio.play();
	window.requestAnimationFrame(frame);
}

</script>

					</div>
					<div id='text'>
						<div id='title'> Fluid Dynamic Music Visualizer</div>
						<div id='names'>Steven Cen, Haoran Guo, Philip Green</div>
						<div class='header1'>Abstract</div>
						<div class='abstracttext'>Our project aims to create a realistic looking 2D simulation of smoke that visualizes an audio input. This class was all about how computers recreate things in the visual sense, so we wanted to extend this theme into the auditory sense and incorporate music, which led us to the goal of visualizing music through tweaking parameters of a smoke simulator. We used TWGL.js and the Web Audio API to implement our simulation shaders and render them onto a browser window, using numerical approximations to solve systems of equations involving smoke density, velocity, and pressure. These numerical methods involve Jacobi iterations as well as projections of velocity fields into irrotational and solenoidal counterparts to maintain mass conversation. To integrate our simulation with audio input, we take in an audio file and process it with a fourier transform to obtain the strength of the frequencies that make up the audio. Using this information, we tweak parameters and inject mass and velocities to our simulation. Doing this with the Web Audio API allows for us to sync the simulation with the music, creating an interesting audio and visual experience. <br><br> To run the simulation simply upload any audio file on the left. Click in the simulation to inject velocity. <br><br> V see more about the about below V</div>	
						<div class='abstracttext'>Our project aims to create a realistic looking 2D simulation of smoke that visualizes an audio input. This class was all about how computers recreate things in the visual sense, so we wanted to extend this theme into the auditory sense and incorporate music, which led us to the goal of visualizing music through tweaking parameters of a smoke simulator. We used TWGL.js and the Web Audio API to implement our simulation shaders and render them onto a browser window, using numerical approximations to solve systems of equations involving smoke density, velocity, and pressure. These numerical methods involve Jacobi iterations as well as projections of velocity fields into irrotational and solenoidal counterparts to maintain mass conversation. To integrate our simulation with audio input, we take in an audio file and process it with a fourier transform to obtain the strength of the frequencies that make up the audio. Using this information, we tweak parameters and inject mass and velocities to our simulation. Doing this with the Web Audio API allows for us to sync the simulation with the music, creating an interesting audio and visual experience. <br><br> To run the simulation simply upload any audio file on the left. Click in the simulation to inject velocity.</div>
					</div>
				</div>
			</div>

			<div id="sec2" class="section">
				<div class='col'>
					<div class='heading'> Technical Approach</div>
					<div class='maintext'>One of the foundational aspects of our fluid dynamics system involves solving many large linear systems that are infeasible for us to calculate directly. Instead, we numerically approximate these equations. The main equation for us to consider is the Navier-Stokes equations which model the motion of viscous fluids:
					</div>
					<img class='img1' src = 'assets/img1.png'/>
					<div class = 'maintext'>From Stamâ€™s paper, Real-Time Fluid Dynamics for Games, we implemented approximate calculations for these equations for a real-time smoke simulation. In fact, the second equation is stated to allow for another approximation to achieve greater speed. Rather than model many particles, the paper suggests a density field which translates into colors that represent the presence of smoke. 
The main movements of the smoke densities are from diffusion and advection. Diffusion involves density from one cell to diffuse into its neighbors, while its neighbors also diffuse density into that cell. A simple implementation can result in diverging values, so a stable one that we used from the paper is to find values accurately line up with the previous time steps densities had we reversed the diffusion by a time step: 
</div>
				<img class='img2' src = 'assets/img2.png'/>

				<div class = 'maintext'>
					To solve for the x values of the next time step Gauss-Seidel relaxation is used. This is a simpler method than matrix inversion for solving our linear system, which is sparse since there are many cells with no smoke density, and thus are zero-valued. <br><br>
We then used the Projection Method, using the principles of the Helmholtz decomposition, whereby any vector field can be represented as the sum of a solenoidal and irrotational vector field. By calculating pressure at all pixels, we were able to generate the irrotational field, and using that could easily find the mass-conserving vector field. <br><br>
We encountered many problems with the Three.js and WebGL as we grappled with learning how to use the frameworks. We also learned the harsh truth about debugging with WebGL, namely that there is no way to debug, other than changing one line at a time and seeing if the output still holds up.

				</div>
					

				</div>
				<div class='col'>
					
				</div>
			</div>

			<div id="sec3" class="section">
				<div class='col'>
					<div class = 'heading'> Results</div>
					<div class = 'maintext'> Please try the simulation on the first page!</div>
					<div class = 'heading'> References</div>
					<div class = 'maintext'> Stam, J., n.d. Real-Time Fluid Dynamics For Games. [ebook] Available at: &lt;https://pdfs.semanticscholar.org/847f/819a4ea14bd789aca8bc88e85e906cfc657c.pdf&gt; [Accessed 13 May 2020].</div>
					<div class = 'heading'> Contributions from each team member </div>
					<div class = 'maintext'>Philip Green: <br>
					Primary coding for the TinyGL implementation of the project, as well as general pipeline planning and music processing.<br>
					Haoran Guo:<br>
					Worked on the WebGL implementation, writing some of the shaders and rendering pipeline. Created the websites for the project deliverables<br>
					Steven Cen:
					Steven conceptualized the general idea of the project, worked on a lot of the WebGL implementation. Debugged and tweaked the final visualization.
</div>
				</div>
				
			</div>		


			</div>
		</div>
	</body>
</html>
